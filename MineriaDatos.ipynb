{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWG3D9NYV_N1"
   },
   "source": [
    "<p class=\"float-right\">![logo](https://github.com/mmattamala/LogosFCFM/blob/master/Ciencias%20de%20la%20Computaci%C3%B3n/Logos%20Facultad/fcfm_dcc_png.png?raw=true =300x)</p>\n",
    "\n",
    "# Informe Minería de Datos\n",
    "\n",
    "CC5206 - Introducción a la Mineria de Datos \n",
    "\n",
    "Profesora: Bárbara Poblete\n",
    "\n",
    "Alumnos: \\\n",
    "Cristián Tamblay \\\n",
    "José Cañete \\\n",
    "Diego Díaz\n",
    "                  \n",
    "\n",
    "Auxiliares: \\\n",
    "Hernán Sarmiento \\\n",
    "José Miguel Herrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxm3CGHVXISJ"
   },
   "source": [
    "#Hito 1\n",
    "###Motivación: \n",
    "\n",
    "¿Cuál es el contexto general del tema/problema/datos que eligió\n",
    "estudiar?\n",
    "\n",
    "- El contexto general son las competencias de Programación Competitiva, éstas son competencias en las que programadores deben pensar y resolver un set de problemas de manera correcta (es decir, eficaces y con soluciones óptimas dado el tamaño del input) y en el menor tiempo posible. En ellas, como el nombre lo dice, se compite entre equipos, quedando en mejor lugar quien tiene más problemas resueltos y en caso de haber varios equipos con la misma cantidad de problemas resueltos, el que haya sido más rápido programando sus soluciones, es decir, que su tiempo acumulado sea menor.\n",
    "Ahora bien, se trabajará con problemas de estas competencias con el fin de caracterizarlos y clasificarlos. Se intentará separar los problemas en categorías de temas, en particular en cómo se resuelven (por ejemplo: programación dinámica y búsqueda binaria) y eventualmente por dificultad del problema. Lo que se intentará es crear un clasificador de estos problemas y en lo posible un recomendador de ellos. Los datos que se eligieron para esta ocasión corresponden a un dataset de problemas de Codeforces.\n",
    "\n",
    "\n",
    "¿Por qué podría ser de interés estos datos?\n",
    "\n",
    "- En primer lugar porque las competencias de Programación Competitiva son muy populares en muchas partes del mundo en contextos en los que se estudia programación, ciencias de la computación y matemáticas. Tanto es así que existen varias competencias a nivel mundial de esto, en particular la IOI a nivel escolar y la ACM ICPC a nivel universitario. Estas competencias tienen un nivel de dificultad alto y para alcanzarlo, estudiantes de todo el mundo se preparan y entrenan constantemente resolviendo problemas. Por otra parte, es conocido que las grandes empresas tech como Google, Microsoft y Facebook entrevistan a sus aspirantes mediante la resolución de problemas de este tipo. Dado ese contexto, el crear un clasificador para de alguna manera facilitar este tipo de entrenamientos puede ser muy util.\n",
    "Por otra parte, es interesante ver si hay alguna relación entre el texto que explica el problema, y el problema computacional que se quiere resolver. Esto debido a que en general los problemas cuentan una historia, que puede ser muy variada en su temática. Además, se podría tratar de encontrar alguna relación con el nivel de dificultad de éste. \n",
    "Por último, también es interesante notar que si bien los problemas pueden categorizarse de manera individual en clases distintas, también pueden categorizarse en más de una al mismo tiempo. El ejemplo clásico de esto son aquellos problemas más avanzados y difíciles en los que las técnicas más básicas (como búsqueda binaria) se usan de manera constante y sin ser el foco principal del problema.\n",
    "\n",
    "###Temática o problemática central:\n",
    "¿Qué es lo que les gustaría analizar utilizando estos datos? o ¿Qué problema les gustaría resolver utilizando estos datos?\n",
    "\n",
    "- Encontrar relaciones entre el texto que describe el problema (y que se cuenta como una historia en la mayoría de los casos) y el problema computacional a resolver, o al menos a qué tipo (tema) de problema corresponde. Para ello el foco estaría en analizar tres aspectos: el texto que describe el problema, el que describe el input y el que describe el output. También aprovecharíamos el hecho de que muchos problemas ya están clasificados. Todo esto sería la base para hacer el clasificador de temas y así, eventualmente poder usar la clasificación anterior en conjunto con las estadísticas de resolución de los problemas, para hacer un clasificador de dificultad y un recomendador de problemas. Una hipótesis inicial en cuanto a la clasificación de temas, es que existen palabras claves que logran describir ciertos temas (por ejemplo, si se menciona \"buscar\" \"máximo\" \"tal que\" \"orden\" es probable que el problema se resuelva con búsqueda binaria).\n",
    "\n",
    "¿Cómo se se puede hacer esto de forma preliminar?\n",
    "\n",
    "- De forma preliminar, lo que se espera es hacer análisis de texto para encontrar patrones, palabras claves, etc. Para ello lo primero planteado es hacer un modelo Bag of Words con los textos que describen el problema, el input y el output con el fin de obtener características y que éstas se puedan procesar de mejor manera. Además de esto, se podrían usar otras técnicas que permitan obtener más características, como lo son el uso de n-gramas o análisis de sentimiento.\n",
    "\n",
    "###Descripción de los datos que se van a utilizar (análisis exploratorio):\n",
    " \n",
    " - Las principales características del dataset son: \n",
    "  - El texto en forma de historia que introduce el problema\n",
    "  - Las especificaciones del input\n",
    "  - Las especificaciones del output\n",
    "  - El límite de memoria\n",
    "  - El límite de tiempo\n",
    "  - Los tags asociados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZEOgMIDRkEt"
   },
   "source": [
    "#### Cosas Útiles\n",
    "\n",
    "Instalación de librerías necesarias para poder visualizar los gráficos que contienen características del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_FENbV2P1yS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='cristian.tamblay', api_key='OJwr2lNHDmoskz1vzRTb')\n",
    "\n",
    "def mostrar_todo(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fK5g_oFORoeD"
   },
   "source": [
    "####Importar Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1IGN9FUVgxK"
   },
   "source": [
    "Importar los datos adjuntos con el informe al ejecutar la siguiente ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "xNnW4kHFOWAw",
    "outputId": "e9f6a4ca-2eef-41c8-a461-ae99e0732284"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.colab import files\\nimport os\\nfiles.upload()\\n!ls'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from google.colab import files\n",
    "import os\n",
    "files.upload()\n",
    "!ls'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2UOudZjvMiE"
   },
   "source": [
    "Verificar que output_all.csv esté en el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFm66nSoRraR"
   },
   "source": [
    "# Leer/importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "uH91nrPCPzu-",
    "outputId": "d5f05d3b-7848-4bc4-d3dd-56f24b378dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3788, 9)\n"
     ]
    }
   ],
   "source": [
    "datos_problemas = pd.read_csv(\"output_all.csv\")\n",
    "datos_problemas.head()\n",
    "\n",
    "#mostrar_todo(datos_problemas.main_text)\n",
    "print(datos_problemas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-xESNl1nUQ9"
   },
   "source": [
    "# Arreglar formato de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es eliminar los problemas que no poseen descripción (main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3773, 9)\n"
     ]
    }
   ],
   "source": [
    "datos_problemas = datos_problemas[pd.notnull(datos_problemas['main_text'])]\n",
    "print(datos_problemas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego arreglamos los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datos_problemas.tags = datos_problemas.tags.apply(lambda x: [str(elem.strip()).replace(\"'\", \"\") for elem in x[1:len(x)-1].split('/')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente que haremos es separar el dataset en dos: una parte que posee al menos un tag y otra que no posee tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_notageados = datos_problemas[datos_problemas.tags.apply(lambda x: (len(x) == 1 and '' in x))]\n",
    "datos_tageados = datos_problemas[datos_problemas.tags.apply(lambda x: not(len(x) == 1 and '' in x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo que haremos es encontrar el set de categorias y luego armaremos un dataframe separado por categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias = []\n",
    "for fila in datos_tageados.tags:\n",
    "    for cat in fila:\n",
    "        categorias.append(cat)\n",
    "categorias = list(set(categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un nuevo dataframe con columnas de cada categoria que contienen un 0 o un 1 dependiendo de si ese texto pertenece a esa categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main_text','tags'] + categorias\n",
    "#print(columns)\n",
    "datos_por_categoria = pd.DataFrame(columns=columns)\n",
    "#datos_por_categoria.head()\n",
    "datos_por_categoria.main_text = datos_tageados.main_text\n",
    "datos_por_categoria.tags = datos_tageados.tags\n",
    "\n",
    "#datos_por_categoria.head()\n",
    "\n",
    "datos_por_categoria.fillna(0)\n",
    "\n",
    "for columna in datos_por_categoria.columns:\n",
    "    if (columna != 'main_text' and columna != 'tags'):\n",
    "        for index, fila in datos_por_categoria.iterrows():\n",
    "            if (columna in fila['tags']):\n",
    "                fila[columna] = 1\n",
    "            else:\n",
    "                fila[columna] = 0\n",
    "\n",
    "datos_por_categoria.drop('tags', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis preliminar de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6XOHgEqc81U"
   },
   "source": [
    "Analizaremos ciertas características del dataset de problemas, partiendo por ver cúanto es el tiempo límite promedio en que deben resolverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "id": "uoUb9CZvQlAb",
    "outputId": "4b0c4def-4395-4d3e-d8d0-7a3f838cfbac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristian.tamblay/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cantidad = []\n",
    "for i in range(0,len(datos_problemas.time_limit.unique())):\n",
    "  cantidad.append(len(datos_problemas[datos_problemas.time_limit == datos_problemas['time_limit'].unique()[i]]))\n",
    "cantidadPorTiempo = np.column_stack((np.asarray(cantidad),datos_problemas['time_limit'].unique()))\n",
    "cantidadPorTiempo=cantidadPorTiempo[(-cantidadPorTiempo[:,0]).argsort()] #El menos es decreasing\n",
    "trace0 = go.Bar(\n",
    "            x=cantidadPorTiempo[:,1],\n",
    "            y=cantidadPorTiempo[:,0]\n",
    "    )\n",
    "data=[trace0]\n",
    "layout = go.Layout(\n",
    "            title='Cantidad de problemas en función del tiempo máximo',\n",
    "            xaxis=dict(\n",
    "                type='category',\n",
    "                title='Tiempo máximo para resolver el problema'\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='Cantidad de problemas'\n",
    "            )\n",
    ")\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='tiempo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_CqStMYdDbC"
   },
   "source": [
    "Se observa que la mayor parte de este dataset se divide entre los problemas de 1 y 2 segundos, lo que no nos aporta mucho sobre la clasificación de los problemas. Pueden haber problemas de temas muy distintos con el mismo tiempo de resolución. \\\n",
    "Haciendo el mismo análisis, pero esta vez midiendo la cantidad de memoria que utilizan los problemas, se llega a lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "id": "jD2aJ69heJUp",
    "outputId": "8339cada-8063-432e-ef6a-831e5bd1e051"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristian.tamblay/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cantidad = []\n",
    "for i in range(0,len(datos_problemas.memory_limit.unique())):\n",
    "  cantidad.append(len(datos_problemas[datos_problemas.memory_limit == datos_problemas['memory_limit'].unique()[i]]))\n",
    "cantidadPorMemoria = np.column_stack((np.asarray(cantidad),datos_problemas['memory_limit'].unique()))\n",
    "cantidadPorMemoria=cantidadPorMemoria[(-cantidadPorMemoria[:,0]).argsort()] #El menos es decreasing\n",
    "trace0 = go.Bar(\n",
    "            x=cantidadPorMemoria[:,1],\n",
    "            y=cantidadPorMemoria[:,0]\n",
    "    )\n",
    "data=[trace0]\n",
    "layout = go.Layout(\n",
    "            title='Cantidad de problemas en función de la cantidad máxima de memoria disponible',\n",
    "            xaxis=dict(\n",
    "                type='category',\n",
    "                title='Cantidad máxima de memoria para resolver el problema'\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='Cantidad de problemas'\n",
    "            )\n",
    ")\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='memoria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klmdqCOmekFt"
   },
   "source": [
    "Se observa que casi todos los problemas tienen como límite los 256 MB de memoria. No se puede encontrar una rapida relación entre el tipo de problema y la cantidad de memoria entregada.\n",
    "\n",
    "A continuación se observan los 30 tags más usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "tT_ZUJEDVC88",
    "outputId": "3dcdcd45-d56e-4f76-eb6f-00207d227862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['math' 'implementation' 'geometry' 'dp' 'number theory'\n",
      " 'constructive algorithms' 'special problem' 'strings' 'data structures'\n",
      " 'dfs and similar' 'greedy' 'binary search' 'probabilities' 'sortings'\n",
      " 'trees' 'divide and conquer' 'brute force' 'dsu' 'graphs' 'flows'\n",
      " 'graph matchings' 'chinese remainder theorem' 'ternary search'\n",
      " 'combinatorics' 'bitmasks' 'matrices' '' 'hashing' 'expression parsing'\n",
      " 'games' 'two pointers' 'string suffix structures' 'shortest paths'\n",
      " '2-sat' 'meet-in-the-middle' 'fft' 'schedules']\n",
      "37\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-23f1dfc243eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposibles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposibles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mcantidad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_problemas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatos_problemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposibles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcantidadPorTag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcantidad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatos_problemas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcantidadPorTag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcantidadPorTag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcantidadPorTag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#El menos es decreasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not list"
     ]
    }
   ],
   "source": [
    "cantidad = []\n",
    "posibles = []\n",
    "for row in datos_problemas.tags:\n",
    "    for tag in row:\n",
    "        posibles.append(tag)\n",
    "posibles = pd.Series(posibles).unique()\n",
    "print(posibles)\n",
    "print(len(posibles))\n",
    "for i in range(0,len(posibles)):\n",
    "  cantidad.append(len(datos_problemas[datos_problemas.tags.tolist() in posibles[i]]))\n",
    "cantidadPorTag = np.column_stack((np.asarray(cantidad),datos_problemas['tags'].unique()))\n",
    "cantidadPorTag=cantidadPorTag[(-cantidadPorTag[:,0]).argsort()] #El menos es decreasing\n",
    "cantidadPorTag=cantidadPorTag[0:10,:]\n",
    "trace0 = go.Bar(\n",
    "            x=cantidadPorTag[:,1],\n",
    "            y=cantidadPorTag[:,0]\n",
    "    )\n",
    "data=[trace0]\n",
    "layout = go.Layout(\n",
    "            title='Cantidad de problemas en función del tag',\n",
    "            xaxis=dict(\n",
    "                type='category',\n",
    "                title='Tag del problema'\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='Cantidad de problemas'\n",
    "            )\n",
    ")\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUnGlOFAa3ln"
   },
   "source": [
    "Se ve que la mayoría de los problemas tiene como tag \"implementation\", lo cual no dice mucho del problema. Adicionalmente, la segunda categoría más popular de problema son aquellos que nisiquiera poseen un tag descriptor, quedando ambigua la naturaleza de éste. Finalmente se puede observar que existen categorías que se repiten, pues están listadas con categorías adicionales, lo cual puede provocar una duplicación de problemas en caso de querer individualizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ordyeGnf6uT2"
   },
   "source": [
    "# Exploración Preliminar del Texto\n",
    "\n",
    "En primer lugar, dado que la mayoría de la información deseamos obtenerla del texto (principal, de input y de output) se decidió hacer uso de la librería SKLearn que entre sus variados métodos ofrece la posibilidad de obtener Bag of Words a partir de texto así como N-Gramas. En este caso particular se decidió probar el uso de Bag of Words de manera sencilla para ver que tal funciona. Como resultado, con el siguiente código se puede ver el Bag of Words creado, donde cada texto inicial se convierte en un vector y podemos ver el vocabulario con las frecuencias de cada palabra en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(datos_por_categoria, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.main_text\n",
    "X_test = test.main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "liUEdHE-xnkZ",
    "outputId": "fffb7b4b-d8ea-4f7d-d85d-08dd35cd9ff5"
   },
   "source": [
    "# Clasificación con Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing constructive algorithms\n",
      "Test accuracy is 0.8960396039603961\n",
      "... Processing fft\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing chinese remainder theorem\n",
      "Test accuracy is 0.9983498349834984\n",
      "... Processing combinatorics\n",
      "Test accuracy is 0.943069306930693\n",
      "... Processing shortest paths\n",
      "Test accuracy is 0.9801980198019802\n",
      "... Processing geometry\n",
      "Test accuracy is 0.9488448844884488\n",
      "... Processing ternary search\n",
      "Test accuracy is 0.9933993399339934\n",
      "... Processing string suffix structures\n",
      "Test accuracy is 0.9892739273927392\n",
      "... Processing dsu\n",
      "Test accuracy is 0.9711221122112211\n",
      "... Processing games\n",
      "Test accuracy is 0.9785478547854786\n",
      "... Processing implementation\n",
      "Test accuracy is 0.7046204620462047\n",
      "... Processing bitmasks\n",
      "Test accuracy is 0.9595709570957096\n",
      "... Processing meet-in-the-middle\n",
      "Test accuracy is 0.995049504950495\n",
      "... Processing special problem\n",
      "Test accuracy is 0.9694719471947195\n",
      "... Processing schedules\n",
      "Test accuracy is 0.9991749174917491\n",
      "... Processing dp\n",
      "Test accuracy is 0.7764026402640264\n",
      "... Processing graph matchings\n",
      "Test accuracy is 0.9942244224422442\n",
      "... Processing math\n",
      "Test accuracy is 0.8226072607260726\n",
      "... Processing dfs and similar\n",
      "Test accuracy is 0.9092409240924092\n",
      "... Processing two pointers\n",
      "Test accuracy is 0.9595709570957096\n",
      "... Processing binary search\n",
      "Test accuracy is 0.9108910891089109\n",
      "... Processing sortings\n",
      "Test accuracy is 0.91996699669967\n",
      "... Processing matrices\n",
      "Test accuracy is 0.9851485148514851\n",
      "... Processing greedy\n",
      "Test accuracy is 0.8250825082508251\n",
      "... Processing divide and conquer\n",
      "Test accuracy is 0.9843234323432343\n",
      "... Processing flows\n",
      "Test accuracy is 0.9843234323432343\n",
      "... Processing probabilities\n",
      "Test accuracy is 0.9686468646864687\n",
      "... Processing data structures\n",
      "Test accuracy is 0.8531353135313532\n",
      "... Processing number theory\n",
      "Test accuracy is 0.9496699669966997\n",
      "... Processing 2-sat\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing trees\n",
      "Test accuracy is 0.9306930693069307\n",
      "... Processing brute force\n",
      "Test accuracy is 0.8481848184818482\n",
      "... Processing expression parsing\n",
      "Test accuracy is 0.9884488448844885\n",
      "... Processing hashing\n",
      "Test accuracy is 0.9826732673267327\n",
      "... Processing graphs\n",
      "Test accuracy is 0.9240924092409241\n",
      "... Processing strings\n",
      "Test accuracy is 0.9447194719471947\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "for category in categorias:\n",
    "    #print(category)\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    #print(train[category].shape)\n",
    "    NB_pipeline.fit(X_train, np.asarray(train[category], dtype=\"|S6\"))\n",
    "    # compute the testing accuracy\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(np.asarray(test[category], dtype=\"|S6\"), prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing constructive algorithms\n",
      "Test accuracy is 0.8927392739273927\n",
      "... Processing fft\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing chinese remainder theorem\n",
      "Test accuracy is 0.9983498349834984\n",
      "... Processing combinatorics\n",
      "Test accuracy is 0.943069306930693\n",
      "... Processing shortest paths\n",
      "Test accuracy is 0.9801980198019802\n",
      "... Processing geometry\n",
      "Test accuracy is 0.9537953795379538\n",
      "... Processing ternary search\n",
      "Test accuracy is 0.9933993399339934\n",
      "... Processing string suffix structures\n",
      "Test accuracy is 0.9892739273927392\n",
      "... Processing dsu\n",
      "Test accuracy is 0.971947194719472\n",
      "... Processing games\n",
      "Test accuracy is 0.9818481848184818\n",
      "... Processing implementation\n",
      "Test accuracy is 0.683993399339934\n",
      "... Processing bitmasks\n",
      "Test accuracy is 0.9587458745874587\n",
      "... Processing meet-in-the-middle\n",
      "Test accuracy is 0.995049504950495\n",
      "... Processing special problem\n",
      "Test accuracy is 0.9760726072607261\n",
      "... Processing schedules\n",
      "Test accuracy is 0.9991749174917491\n",
      "... Processing dp\n",
      "Test accuracy is 0.7739273927392739\n",
      "... Processing graph matchings\n",
      "Test accuracy is 0.9942244224422442\n",
      "... Processing math\n",
      "Test accuracy is 0.806930693069307\n",
      "... Processing dfs and similar\n",
      "Test accuracy is 0.905940594059406\n",
      "... Processing two pointers\n",
      "Test accuracy is 0.9570957095709571\n",
      "... Processing binary search\n",
      "Test accuracy is 0.9092409240924092\n",
      "... Processing sortings\n",
      "Test accuracy is 0.9133663366336634\n",
      "... Processing matrices\n",
      "Test accuracy is 0.9851485148514851\n",
      "... Processing greedy\n",
      "Test accuracy is 0.7929042904290429\n",
      "... Processing divide and conquer\n",
      "Test accuracy is 0.9843234323432343\n",
      "... Processing flows\n",
      "Test accuracy is 0.9851485148514851\n",
      "... Processing probabilities\n",
      "Test accuracy is 0.9694719471947195\n",
      "... Processing data structures\n",
      "Test accuracy is 0.8605610561056105\n",
      "... Processing number theory\n",
      "Test accuracy is 0.9496699669966997\n",
      "... Processing 2-sat\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing trees\n",
      "Test accuracy is 0.9372937293729373\n",
      "... Processing brute force\n",
      "Test accuracy is 0.8374587458745875\n",
      "... Processing expression parsing\n",
      "Test accuracy is 0.9884488448844885\n",
      "... Processing hashing\n",
      "Test accuracy is 0.9826732673267327\n",
      "... Processing graphs\n",
      "Test accuracy is 0.9249174917491749\n",
      "... Processing strings\n",
      "Test accuracy is 0.943069306930693\n"
     ]
    }
   ],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "for category in categorias:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    SVC_pipeline.fit(X_train, np.asarray(train[category], dtype=\"|S6\"))\n",
    "    # compute the testing accuracy\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(np.asarray(test[category], dtype=\"|S6\"), prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing constructive algorithms\n",
      "Test accuracy is 0.8960396039603961\n",
      "... Processing fft\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing chinese remainder theorem\n",
      "Test accuracy is 0.9983498349834984\n",
      "... Processing combinatorics\n",
      "Test accuracy is 0.943069306930693\n",
      "... Processing shortest paths\n",
      "Test accuracy is 0.9801980198019802\n",
      "... Processing geometry\n",
      "Test accuracy is 0.9496699669966997\n",
      "... Processing ternary search\n",
      "Test accuracy is 0.9933993399339934\n",
      "... Processing string suffix structures\n",
      "Test accuracy is 0.9892739273927392\n",
      "... Processing dsu\n",
      "Test accuracy is 0.9711221122112211\n",
      "... Processing games\n",
      "Test accuracy is 0.9785478547854786\n",
      "... Processing implementation\n",
      "Test accuracy is 0.7087458745874587\n",
      "... Processing bitmasks\n",
      "Test accuracy is 0.9595709570957096\n",
      "... Processing meet-in-the-middle\n",
      "Test accuracy is 0.995049504950495\n",
      "... Processing special problem\n",
      "Test accuracy is 0.9694719471947195\n",
      "... Processing schedules\n",
      "Test accuracy is 0.9991749174917491\n",
      "... Processing dp\n",
      "Test accuracy is 0.7764026402640264\n",
      "... Processing graph matchings\n",
      "Test accuracy is 0.9942244224422442\n",
      "... Processing math\n",
      "Test accuracy is 0.8217821782178217\n",
      "... Processing dfs and similar\n",
      "Test accuracy is 0.9092409240924092\n",
      "... Processing two pointers\n",
      "Test accuracy is 0.9595709570957096\n",
      "... Processing binary search\n",
      "Test accuracy is 0.9108910891089109\n",
      "... Processing sortings\n",
      "Test accuracy is 0.91996699669967\n",
      "... Processing matrices\n",
      "Test accuracy is 0.9851485148514851\n",
      "... Processing greedy\n",
      "Test accuracy is 0.8250825082508251\n",
      "... Processing divide and conquer\n",
      "Test accuracy is 0.9843234323432343\n",
      "... Processing flows\n",
      "Test accuracy is 0.9843234323432343\n",
      "... Processing probabilities\n",
      "Test accuracy is 0.9686468646864687\n",
      "... Processing data structures\n",
      "Test accuracy is 0.8547854785478548\n",
      "... Processing number theory\n",
      "Test accuracy is 0.9504950495049505\n",
      "... Processing 2-sat\n",
      "Test accuracy is 0.9966996699669967\n",
      "... Processing trees\n",
      "Test accuracy is 0.9356435643564357\n",
      "... Processing brute force\n",
      "Test accuracy is 0.8481848184818482\n",
      "... Processing expression parsing\n",
      "Test accuracy is 0.9884488448844885\n",
      "... Processing hashing\n",
      "Test accuracy is 0.9826732673267327\n",
      "... Processing graphs\n",
      "Test accuracy is 0.9273927392739274\n",
      "... Processing strings\n",
      "Test accuracy is 0.9496699669966997\n"
     ]
    }
   ],
   "source": [
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "for category in categorias:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    LogReg_pipeline.fit(X_train, np.asarray(train[category], dtype=\"|S6\"))\n",
    "    # compute the testing accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(np.asarray(test[category], dtype=\"|S6\"), prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros códigos eventualmente utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPLETAR ESTE CÓDIGO\n",
    "\n",
    "## run_classifier recibe un clasificador, un dataset (X, y) \n",
    "## y opcionalmente la cantidad de resultados que se quiere obtener del clasificador\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "def run_classifier(clf, X, y, num_tests=100):\n",
    "    metrics = {'f1-score': [], 'precision': [], 'recall': []}\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        \n",
    "        ### INICIO COMPLETAR ACÁ \n",
    "        #### TIP: divida el dataset, entrene y genere las predicciones.\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, stratify=y)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "      \n",
    "        ### FIN COMPLETAR ACÁ\n",
    "        \n",
    "        \n",
    "        metrics['f1-score'].append(f1_score(y_test, predictions))  # X_test y y_test deben ser definidos previamente\n",
    "        metrics['recall'].append(recall_score(y_test, predictions))\n",
    "        metrics['precision'].append(precision_score(y_test, predictions))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ejecutar este código\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC  # support vector machine classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB  # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bc = load_breast_cancer()    # dataset cancer de mamas\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "c0 = (\"Base Dummy\", DummyClassifier(strategy='stratified'))\n",
    "c1 = (\"Decision Tree\", DecisionTreeClassifier())\n",
    "c2 = (\"Gaussian Naive Bayes\", GaussianNB())\n",
    "c3 = (\"KNN\", KNeighborsClassifier(n_neighbors=5))\n",
    "\n",
    "classifiers = [c0, c1, c2, c3]\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    metrics = run_classifier(clf, X, y)   # hay que implementarla en el bloque anterior.\n",
    "    print(\"----------------\")\n",
    "    print(\"Resultados para clasificador: \",name) \n",
    "    print(\"Precision promedio:\",np.array(metrics['precision']).mean())\n",
    "    print(\"Recall promedio:\",np.array(metrics['recall']).mean())\n",
    "    print(\"F1-score promedio:\",np.array(metrics['f1-score']).mean())\n",
    "    print(\"----------------\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MineriaDatos.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
